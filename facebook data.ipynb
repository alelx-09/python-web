{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import os\n",
    "import platform\n",
    "import sys\n",
    "import urllib.request\n",
    "import datetime, time, csv\n",
    "\n",
    "from sys import argv\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Global Variables\n",
    "\n",
    "driver = None\n",
    "\n",
    "# whether to download photos or not\n",
    "download_uploaded_photos = True\n",
    "download_friends_photos = True\n",
    "\n",
    "# whether to download the full image or its thumbnail (small size)\n",
    "# if small size is True then it will be very quick else if its false then it will open each photo to download it\n",
    "# and it will take much more time\n",
    "friends_small_size = True\n",
    "photos_small_size = True\n",
    "\n",
    "total_scrolls = 5000\n",
    "current_scrolls = 0\n",
    "scroll_time = 5\n",
    "\n",
    "old_height = 0\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def get_facebook_images_url(img_links):\n",
    "    urls = []\n",
    "\n",
    "    for link in img_links:\n",
    "\n",
    "        if link != \"None\":\n",
    "            valid_url_found = False\n",
    "            driver.get(link)\n",
    "\n",
    "            try:\n",
    "                while not valid_url_found:\n",
    "                    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"spotlight\")))\n",
    "                    element = driver.find_element_by_class_name(\"spotlight\")\n",
    "                    img_url = element.get_attribute('src')\n",
    "\n",
    "                    if img_url.find('.gif') == -1:\n",
    "                        valid_url_found = True\n",
    "                        urls.append(img_url)\n",
    "\n",
    "            except EC.StaleElementReferenceException:\n",
    "                urls.append(driver.find_element_by_class_name(\"spotlight\").get_attribute('src'))\n",
    "\n",
    "            except:\n",
    "                print(\"Exception (facebook_image_downloader):\", sys.exc_info()[0])\n",
    "\n",
    "        else:\n",
    "            urls.append(\"None\")\n",
    "\n",
    "    return urls\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# takes a url and downloads image from that url\n",
    "def image_downloader(img_links, folder_name):\n",
    "    img_names = []\n",
    "\n",
    "    try:\n",
    "        parent = os.getcwd()\n",
    "        try:\n",
    "            folder = os.path.join(os.getcwd(), folder_name)\n",
    "            if not os.path.exists(folder):\n",
    "                os.mkdir(folder)\n",
    "\n",
    "            os.chdir(folder)\n",
    "        except:\n",
    "            print(\"Error in changing directory\")\n",
    "\n",
    "        for link in img_links:\n",
    "            img_name = \"None\"\n",
    "\n",
    "            if link != \"None\":\n",
    "                img_name = (link.split('.jpg')[0]).split('/')[-1] + '.jpg'\n",
    "\n",
    "                if img_name == \"10354686_10150004552801856_220367501106153455_n.jpg\":\n",
    "                    img_name = \"None\"\n",
    "                else:\n",
    "                    try:\n",
    "                        urllib.request.urlretrieve(link, img_name)\n",
    "                    except:\n",
    "                        img_name = \"None\"\n",
    "\n",
    "            img_names.append(img_name)\n",
    "\n",
    "        os.chdir(parent)\n",
    "    except:\n",
    "        print(\"Exception (image_downloader):\", sys.exc_info()[0])\n",
    "\n",
    "    return img_names\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def check_height():\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    return new_height != old_height\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# helper function: used to scroll the page\n",
    "def scroll():\n",
    "    global old_height\n",
    "    current_scrolls = 0\n",
    "\n",
    "    while (True):\n",
    "        try:\n",
    "            if current_scrolls == total_scrolls:\n",
    "                return\n",
    "\n",
    "            old_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            WebDriverWait(driver, scroll_time, 0.05).until(lambda driver: check_height())\n",
    "            current_scrolls += 1\n",
    "        except TimeoutException:\n",
    "            break\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# --Helper Functions for Posts\n",
    "\n",
    "def get_status(x):\n",
    "    status = \"\"\n",
    "    try:\n",
    "        status = x.find_element_by_xpath(\".//div[@class='_5pbx userContent']\").text\n",
    "    except:\n",
    "        try:\n",
    "            status = x.find_element_by_xpath(\".//div[@class='userContent']\").text\n",
    "        except:\n",
    "            pass\n",
    "    return status\n",
    "\n",
    "\n",
    "def get_div_links(x, tag):\n",
    "    try:\n",
    "        temp = x.find_element_by_xpath(\".//div[@class='_3x-2']\")\n",
    "        return temp.find_element_by_tag_name(tag)\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def get_title_links(title):\n",
    "    l = title.find_elements_by_tag_name('a')\n",
    "    return l[-1].text, l[-1].get_attribute('href')\n",
    "\n",
    "\n",
    "def get_title(x):\n",
    "    title = \"\"\n",
    "    try:\n",
    "        title = x.find_element_by_xpath(\".//span[@class='fwb fcg']\")\n",
    "    except:\n",
    "        try:\n",
    "            title = x.find_element_by_xpath(\".//span[@class='fcg']\")\n",
    "        except:\n",
    "            try:\n",
    "                title = x.find_element_by_xpath(\".//span[@class='fwn fcg']\")\n",
    "            except:\n",
    "                pass\n",
    "    finally:\n",
    "        return title\n",
    "\n",
    "\n",
    "def get_time(x):\n",
    "    time = \"\"\n",
    "    try:\n",
    "        time = x.find_element_by_tag_name('abbr').get_attribute('title')\n",
    "        time = str(\"%02d\" % int(time.split(\", \")[1].split()[1]), ) + \"-\" + str(\n",
    "            (\"%02d\" % (int((list(calendar.month_abbr).index(time.split(\", \")[1].split()[0][:3]))),))) + \"-\" + \\\n",
    "               time.split()[3] + \" \" + str(\"%02d\" % int(time.split()[5].split(\":\")[0])) + \":\" + str(\n",
    "            time.split()[5].split(\":\")[1])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    finally:\n",
    "        return time\n",
    "\n",
    "\n",
    "def extract_and_write_posts(elements, filename):\n",
    "    try:\n",
    "        f = open(filename, \"w\", newline='\\r\\n')\n",
    "        f.writelines(' TIME || TYPE  || TITLE || STATUS  ||   LINKS(Shared Posts/Shared Links etc) ' + '\\n' + '\\n')\n",
    "\n",
    "        for x in elements:\n",
    "            try:\n",
    "                video_link = \" \"\n",
    "                title = \" \"\n",
    "                status = \" \"\n",
    "                link = \"\"\n",
    "                img = \" \"\n",
    "                time = \" \"\n",
    "\n",
    "                # time\n",
    "                time = get_time(x)\n",
    "\n",
    "                # title\n",
    "                title = get_title(x)\n",
    "                if title.text.find(\"shared a memory\") != -1:\n",
    "                    x = x.find_element_by_xpath(\".//div[@class='_1dwg _1w_m']\")\n",
    "                    title = get_title(x)\n",
    "\n",
    "                status = get_status(x)\n",
    "                if title.text == driver.find_element_by_id(\"fb-timeline-cover-name\").text:\n",
    "                    if status == '':\n",
    "                        temp = get_div_links(x, \"img\")\n",
    "                        if temp == '':  # no image tag which means . it is not a life event\n",
    "                            link = get_div_links(x, \"a\").get_attribute('href')\n",
    "                            type = \"status update without text\"\n",
    "                        else:\n",
    "                            type = 'life event'\n",
    "                            link = get_div_links(x, \"a\").get_attribute('href')\n",
    "                            status = get_div_links(x, \"a\").text\n",
    "                    else:\n",
    "                        type = \"status update\"\n",
    "                        if get_div_links(x, \"a\") != '':\n",
    "                            link = get_div_links(x, \"a\").get_attribute('href')\n",
    "\n",
    "                elif title.text.find(\" shared \") != -1:\n",
    "\n",
    "                    x1, link = get_title_links(title)\n",
    "                    type = \"shared \" + x1\n",
    "\n",
    "                elif title.text.find(\" at \") != -1 or title.text.find(\" in \") != -1:\n",
    "                    if title.text.find(\" at \") != -1:\n",
    "                        x1, link = get_title_links(title)\n",
    "                        type = \"check in\"\n",
    "                    elif title.text.find(\" in \") != 1:\n",
    "                        status = get_div_links(x, \"a\").text\n",
    "\n",
    "                elif title.text.find(\" added \") != -1 and title.text.find(\"photo\") != -1:\n",
    "                    type = \"added photo\"\n",
    "                    link = get_div_links(x, \"a\").get_attribute('href')\n",
    "\n",
    "                elif title.text.find(\" added \") != -1 and title.text.find(\"video\") != -1:\n",
    "                    type = \"added video\"\n",
    "                    link = get_div_links(x, \"a\").get_attribute('href')\n",
    "\n",
    "                else:\n",
    "                    type = \"others\"\n",
    "\n",
    "                if not isinstance(title, str):\n",
    "                    title = title.text\n",
    "\n",
    "                status = status.replace(\"\\n\", \" \")\n",
    "                title = title.replace(\"\\n\", \" \")\n",
    "\n",
    "                line = str(time) + \" || \" + str(type) + ' || ' + str(title) + ' || ' + str(status) + ' || ' + str(\n",
    "                    link) + \"\\n\"\n",
    "\n",
    "                try:\n",
    "                    f.writelines(line)\n",
    "                except:\n",
    "                    print('Posts: Could not map encoded characters')\n",
    "            except:\n",
    "                pass\n",
    "        f.close()\n",
    "    except:\n",
    "        print(\"Exception (extract_and_write_posts)\", \"Status =\", sys.exc_info()[0])\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "\n",
    "def save_to_file(name, elements, status, current_section):\n",
    "    \"\"\"helper function used to save links to files\"\"\"\n",
    "\n",
    "    # status 0 = dealing with friends list\n",
    "    # status 1 = dealing with photos\n",
    "    # status 2 = dealing with videos\n",
    "    # status 3 = dealing with about section\n",
    "    # status 4 = dealing with posts\n",
    "\n",
    "    try:\n",
    "\n",
    "        f = None  # file pointer\n",
    "\n",
    "        if status != 4:\n",
    "            f = open(name, 'w', encoding='utf-8', newline='\\r\\n')\n",
    "\n",
    "        results = []\n",
    "        img_names = []\n",
    "\n",
    "        # dealing with Friends\n",
    "        if status == 0:\n",
    "\n",
    "            results = [x.get_attribute('href') for x in elements]\n",
    "            results = [create_original_link(x) for x in results]\n",
    "\n",
    "            try:\n",
    "                if download_friends_photos:\n",
    "\n",
    "                    if friends_small_size:\n",
    "                        img_links = [x.find_element_by_css_selector('img').get_attribute('src') for x in elements]\n",
    "                    else:\n",
    "                        links = []\n",
    "                        for friend in results:\n",
    "                            driver.get(friend)\n",
    "                            WebDriverWait(driver, 10).until(\n",
    "                                EC.presence_of_element_located((By.CLASS_NAME, \"profilePicThumb\")))\n",
    "                            l = driver.find_element_by_class_name(\"profilePicThumb\").get_attribute('href')\n",
    "                            links.append(l)\n",
    "\n",
    "                        for i in range(len(links)):\n",
    "                            if links[i].find('picture/view') != -1:\n",
    "                                links[i] = \"None\"\n",
    "\n",
    "                        img_links = get_facebook_images_url(links)\n",
    "\n",
    "                    folder_names = [\"Friend's Photos\", \"Following's Photos\", \"Follower's Photos\", \"Work Friends Photos\",\n",
    "                                    \"College Friends Photos\", \"Current City Friends Photos\", \"Hometown Friends Photos\"]\n",
    "                    print(\"Downloading \" + folder_names[current_section])\n",
    "\n",
    "                    img_names = image_downloader(img_links, folder_names[current_section])\n",
    "            except:\n",
    "                print(\"Exception (Images)\", str(status), \"Status =\", current_section, sys.exc_info()[0])\n",
    "\n",
    "        # dealing with Photos\n",
    "        elif status == 1:\n",
    "            results = [x.get_attribute('href') for x in elements]\n",
    "            results.pop(0)\n",
    "\n",
    "            try:\n",
    "                if download_uploaded_photos:\n",
    "                    if photos_small_size:\n",
    "                        background_img_links = driver.find_elements_by_xpath(\"//*[contains(@id, 'pic_')]/div/i\")\n",
    "                        background_img_links = [x.get_attribute('style') for x in background_img_links]\n",
    "                        background_img_links = [((x.split('(')[1]).split(')')[0]).strip('\"') for x in\n",
    "                                                background_img_links]\n",
    "                    else:\n",
    "                        background_img_links = get_facebook_images_url(results)\n",
    "\n",
    "                    folder_names = [\"Uploaded Photos\", \"Tagged Photos\"]\n",
    "                    print(\"Downloading \" + folder_names[current_section])\n",
    "\n",
    "                    img_names = image_downloader(background_img_links, folder_names[current_section])\n",
    "            except:\n",
    "                print(\"Exception (Images)\", str(status), \"Status =\", current_section, sys.exc_info()[0])\n",
    "\n",
    "        # dealing with Videos\n",
    "        elif status == 2:\n",
    "            results = elements[0].find_elements_by_css_selector('li')\n",
    "            results = [x.find_element_by_css_selector('a').get_attribute('href') for x in results]\n",
    "\n",
    "            try:\n",
    "                if results[0][0] == '/':\n",
    "                    results = [r.pop(0) for r in results]\n",
    "                    results = [(\"https://en-gb.facebook.com/\" + x) for x in results]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # dealing with About Section\n",
    "        elif status == 3:\n",
    "            results = elements[0].text\n",
    "            f.writelines(results)\n",
    "\n",
    "        # dealing with Posts\n",
    "        elif status == 4:\n",
    "            extract_and_write_posts(elements, name)\n",
    "            return\n",
    "\n",
    "        if (status == 0) or (status == 1):\n",
    "            for i in range(len(results)):\n",
    "                f.writelines(results[i])\n",
    "                f.write(',')\n",
    "                try:\n",
    "                    f.writelines(img_names[i])\n",
    "                except:\n",
    "                    f.writelines(\"None\")\n",
    "                f.write('\\n')\n",
    "\n",
    "        elif status == 2:\n",
    "            for x in results:\n",
    "                f.writelines(x + \"\\n\")\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    except:\n",
    "        print(\"Exception (save_to_file)\", \"Status =\", str(status), sys.exc_info()[0])\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def scrap_data(id, scan_list, section, elements_path, save_status, file_names):\n",
    "    \"\"\"Given some parameters, this function can scrap friends/photos/videos/about/posts(statuses) of a profile\"\"\"\n",
    "\n",
    "    page = []\n",
    "\n",
    "    if save_status == 4:\n",
    "        page.append(id)\n",
    "\n",
    "    for i in range(len(section)):\n",
    "        page.append(id + section[i])\n",
    "\n",
    "    for i in range(len(scan_list)):\n",
    "        try:\n",
    "            driver.get(page[i])\n",
    "\n",
    "            if (save_status == 0) or (save_status == 1) or (\n",
    "                    save_status == 2):  # Only run this for friends, photos and videos\n",
    "\n",
    "                # the bar which contains all the sections\n",
    "                sections_bar = driver.find_element_by_xpath(\"//*[@class='_3cz'][1]/div[2]/div[1]\")\n",
    "\n",
    "                if sections_bar.text.find(scan_list[i]) == -1:\n",
    "                    continue\n",
    "\n",
    "            if save_status != 3:\n",
    "                scroll()\n",
    "\n",
    "            data = driver.find_elements_by_xpath(elements_path[i])\n",
    "\n",
    "            save_to_file(file_names[i], data, save_status, i)\n",
    "            \n",
    "            #return data[i].text\n",
    "\n",
    "        except:\n",
    "            print(\"Exception (scrap_data)\", str(i), \"Status =\", str(save_status), sys.exc_info()[0])\n",
    "            #return \"\"\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def create_original_link(url):\n",
    "    if url.find(\".php\") != -1:\n",
    "        original_link = \"https://en-gb.facebook.com/\" + ((url.split(\"=\"))[1])\n",
    "\n",
    "        if original_link.find(\"&\") != -1:\n",
    "            original_link = original_link.split(\"&\")[0]\n",
    "\n",
    "    elif url.find(\"fnr_t\") != -1:\n",
    "        original_link = \"https://en-gb.facebook.com/\" + ((url.split(\"/\"))[-1].split(\"?\")[0])\n",
    "    elif url.find(\"_tab\") != -1:\n",
    "        original_link = \"https://en-gb.facebook.com/\" + (url.split(\"?\")[0]).split(\"/\")[-1]\n",
    "    else:\n",
    "        original_link = url\n",
    "\n",
    "    return original_link\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "def scrap_messages():\n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"Messages..\")\n",
    "    try:\n",
    "        scan_list = [\"Messages\"]\n",
    "        section = [\"https://www.facebook.com/messages\"]\n",
    "        elements_path = [\"//*[contains(@id,'js_d')]/div/ul/li/div/a\"]\n",
    "        file_names = [\"Messages.txt\"]\n",
    "        save_status = 0\n",
    "        driver.get(section[0])\n",
    "        ele = driver.find_element_by_id('js_d')\n",
    "        f = open(file_names[0], 'w', encoding='utf-8', newline='\\r\\n')\n",
    "        f.writelines(ele.text +'\\n')\n",
    "        f.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"There's some error, while scraping messages.\")\n",
    "\t\t\n",
    "    driver.get(\"https://www.facebook.com\")\n",
    "    print(\"Messages Done\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def scrap_profile(ids):\n",
    "    folder = os.path.join(os.getcwd(), \"Data\")\n",
    "\n",
    "    if not os.path.exists(folder):\n",
    "        os.mkdir(folder)\n",
    "\n",
    "    os.chdir(folder)\n",
    "    csvOut = 'output.csv'\n",
    "    writer = csv.writer(open(csvOut, 'w'))\n",
    "    writer.writerow(['All Friends','Following','Followers','Work Friends','College Friends','Current City Friends','Hometown Friends','Uploaded Photos','Tagged Photos','Uploaded Videos','Tagged Videos','Overview','Work and Education','Places Lived','Contact and Basic Info','Family and Relationships','Details About','Life Events','Posts'])\n",
    "\n",
    "    # execute for all profiles given in input.txt file\n",
    "    for id in ids:\n",
    "\n",
    "        driver.get(id)\n",
    "        url = driver.current_url\n",
    "        id = create_original_link(url)\n",
    "        mydir = os.path.join(folder, id.split('/')[-1])\n",
    "\n",
    "        print(\"\\nScraping:\", id)\n",
    "\n",
    "        try:\n",
    "            if not os.path.exists(mydir):\n",
    "                os.mkdir(mydir)\n",
    "            else:\n",
    "                print(\"A folder with the same profile name already exists.\"\n",
    "                      \"\\nKindly remove that folder first and then run this code.\")\n",
    "                continue\n",
    "            os.chdir(mydir)\n",
    "        except:\n",
    "            print(\"Some error occurred in creating the profile directory.\")\n",
    "            continue\n",
    "\n",
    "        # ----------------------------------------------------------------------------\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Friends..\")\n",
    "        # setting parameters for scrap_data() to scrap friends\n",
    "        scan_list = [\"All\", \"Following\", \"Followers\", \"Work\", \"College\", \"Current City\", \"Hometown\"]\n",
    "        section = [\"/friends\", \"/following\", \"/followers\", \"/friends_work\", \"/friends_college\", \"/friends_current_city\",\n",
    "                   \"/friends_hometown\"]\n",
    "        elements_path = [\"//*[contains(@id,'pagelet_timeline_medley_friends')][1]/div[2]/div/ul/li/div/a\",\n",
    "                         \"//*[contains(@class,'_3i9')][1]/div/div/ul/li[1]/div[2]/div/div/div/div/div[2]/ul/li/div/a\",\n",
    "                         \"//*[contains(@class,'fbProfileBrowserListItem')]/div/a\",\n",
    "                         \"//*[contains(@id,'pagelet_timeline_medley_friends')][1]/div[2]/div/ul/li/div/a\",\n",
    "                         \"//*[contains(@id,'pagelet_timeline_medley_friends')][1]/div[2]/div/ul/li/div/a\",\n",
    "                         \"//*[contains(@id,'pagelet_timeline_medley_friends')][1]/div[2]/div/ul/li/div/a\",\n",
    "                         \"//*[contains(@id,'pagelet_timeline_medley_friends')][1]/div[2]/div/ul/li/div/a\"]\n",
    "        file_names = [\"All Friends.txt\", \"Following.txt\", \"Followers.txt\", \"Work Friends.txt\", \"College Friends.txt\",\n",
    "                      \"Current City Friends.txt\", \"Hometown Friends.txt\"]\n",
    "        save_status = 0\n",
    "\n",
    "        dataFriends = scrap_data(id, scan_list, section, elements_path, save_status, file_names)\n",
    "        print(\"Friends Done\")\n",
    "        # ----------------------------------------------------------------------------\n",
    "\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Photos..\")\n",
    "        print(\"Scraping Links..\")\n",
    "        # setting parameters for scrap_data() to scrap photos\n",
    "        scan_list = [\"'s Photos\", \"Photos of\"]\n",
    "        section = [\"/photos_all\", \"/photos_of\"]\n",
    "        elements_path = [\"//*[contains(@id, 'pic_')]\"] * 2\n",
    "        file_names = [\"Uploaded Photos.txt\", \"Tagged Photos.txt\"]\n",
    "        save_status = 1\n",
    "\n",
    "        dataPhotos = scrap_data(id, scan_list, section, elements_path, save_status, file_names)\n",
    "        print(\"Photos Done\")\n",
    "\n",
    "        # ----------------------------------------------------------------------------\n",
    "\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Videos:\")\n",
    "        # setting parameters for scrap_data() to scrap videos\n",
    "        scan_list = [\"'s Videos\", \"Videos of\"]\n",
    "        section = [\"/videos_by\", \"/videos_of\"]\n",
    "        elements_path = [\"//*[contains(@id, 'pagelet_timeline_app_collection_')]/ul\"] * 2\n",
    "        file_names = [\"Uploaded Videos.txt\", \"Tagged Videos.txt\"]\n",
    "        save_status = 2\n",
    "\n",
    "        dataVideos = scrap_data(id, scan_list, section, elements_path, save_status, file_names)\n",
    "        print(\"Videos Done\")\n",
    "        # ----------------------------------------------------------------------------\n",
    "\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"About:\")\n",
    "        # setting parameters for scrap_data() to scrap the about section\n",
    "        scan_list = [None] * 7\n",
    "        section = [\"/about?section=overview\", \"/about?section=education\", \"/about?section=living\",\n",
    "                   \"/about?section=contact-info\", \"/about?section=relationship\", \"/about?section=bio\",\n",
    "                   \"/about?section=year-overviews\"]\n",
    "        elements_path = [\"//*[contains(@id, 'pagelet_timeline_app_collection_')]/ul/li/div/div[2]/div/div\"] * 7\n",
    "        file_names = [\"Overview.txt\", \"Work and Education.txt\", \"Places Lived.txt\", \"Contact and Basic Info.txt\",\n",
    "                      \"Family and Relationships.txt\", \"Details About.txt\", \"Life Events.txt\"]\n",
    "        \n",
    "        save_status = 3\n",
    "        #writer.writerow([\"/about?section=overview\", \"/about?section=education\", \"/about?section=living\",\"/about?section=contact-info\", \"/about?section=relationship\", \"/about?section=bio\",\"/about?section=year-overviews\"])\n",
    "        \n",
    "        dataAbout = scrap_data(id, scan_list, section, elements_path, save_status, file_names)\n",
    "        #print(dataAbout)\n",
    "        print(\"About Section Done\")\n",
    "\n",
    "        # ----------------------------------------------------------------------------\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Posts:\")\n",
    "        # setting parameters for scrap_data() to scrap posts\n",
    "        scan_list = [None]\n",
    "        section = []\n",
    "        elements_path = [\"//div[@class='_4-u2 mbm _4mrt _5jmm _5pat _5v3q _4-u8']\"]\n",
    "\n",
    "        file_names = [\"Posts.txt\"]\n",
    "        save_status = 4\n",
    "\n",
    "        dataPost = scrap_data(id, scan_list, section, elements_path, save_status, file_names)\n",
    "        print(\"Posts(Statuses) Done\")\n",
    "        print(\"----------------------------------------\")\n",
    "\n",
    "        dataAllFriends=read_file(mydir+\"\\All Friends.txt\")\n",
    "        dataFollowing=read_file(mydir+\"\\Following.txt\")\n",
    "        dataFollowers=read_file(mydir+\"\\Followers.txt\")\n",
    "        dataWorkFriends=read_file(mydir+\"\\Work Friends.txt\")\n",
    "        dataCollegeFriends=read_file(mydir+\"\\College Friends.txt\")\n",
    "        dataCurrentCityFriends=read_file(mydir+\"\\Current City Friends.txt\")\n",
    "        dataHometownFriends=read_file(mydir+\"\\Hometown Friends.txt\")\n",
    "        \n",
    "        dataUploadedPhotos=read_file(mydir+\"Uploaded Photos.txt\")\n",
    "        #print(mydir+\"Uploaded Photos.txt\")\n",
    "        dataTaggedPhotos=read_file(mydir+\"\\Tagged Photos.txt\")\n",
    "\n",
    "        dataUploadedVideos=read_file(mydir+\"Uploaded Videos.txt\")\n",
    "        dataTaggedVideos=read_file(mydir+\"\\Tagged Videos.txt\")\n",
    "        \n",
    "        dataOverview=read_file(mydir+\"\\Overview.txt\")\n",
    "        dataWork=read_file(mydir+\"\\Work and Education.txt\")\n",
    "        dataPlaces=read_file(mydir+\"\\Places Lived.txt\")\n",
    "        dataContact=read_file(mydir+\"\\Contact and Basic Info.txt\")\n",
    "        dataFamily=read_file(mydir+\"\\Family and Relationships.txt\")\n",
    "        dataDetails=read_file(mydir+\"\\Details About.txt\")\n",
    "        dataLifeEvents=read_file(mydir+\"\\Life Events.txt\")\n",
    "\n",
    "        dataPosts=read_file(mydir+\"\\Posts.txt\")\n",
    "        writer.writerow([dataAllFriends,dataFollowing,dataFollowers,dataWorkFriends,dataCollegeFriends,dataCurrentCityFriends,dataHometownFriends,dataUploadedPhotos,dataTaggedPhotos,dataUploadedVideos,dataTaggedVideos,dataOverview,dataWork,dataPlaces,dataContact,dataFamily,dataDetails,dataLifeEvents,dataPosts])\n",
    "        \n",
    "    # ----------------------------------------------------------------------------\n",
    "\n",
    "    print(\"\\nProcess Completed.\")\n",
    "\n",
    "    return\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def read_file(myfile):\n",
    "    try:\n",
    "        file1 = open(myfile,\"r+\")\n",
    "        data = file1.readlines()\n",
    "        file1.close()\n",
    "        return data\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def login(email, password):\n",
    "    \"\"\" Logging into our own profile \"\"\"\n",
    "\n",
    "    try:\n",
    "        global driver\n",
    "\n",
    "        options = Options()\n",
    "\n",
    "        #  Code to disable notifications pop up of Chrome Browser\n",
    "        options.add_argument(\"--disable-notifications\")\n",
    "        options.add_argument(\"--disable-infobars\")\n",
    "        options.add_argument(\"--mute-audio\")\n",
    "        # options.add_argument(\"headless\")\n",
    "\n",
    "        try:\n",
    "            if platform.system() == 'Linux':\n",
    "                driver = webdriver.Chrome(executable_path=\"./chromedriver\", chrome_options=options)\n",
    "            else:\n",
    "                driver = webdriver.Chrome(executable_path=\"./chromedriver.exe\", chrome_options=options)\n",
    "        except:\n",
    "            print(\"Kindly replace the Chrome Web Driver with the latest one from\"\n",
    "                  \" http://chromedriver.chromium.org/downloads\")\n",
    "            exit()\n",
    "\n",
    "        driver.get(\"https://en-gb.facebook.com\")\n",
    "        driver.maximize_window()\n",
    "\n",
    "        # filling the form\n",
    "        driver.find_element_by_name('email').send_keys(email)\n",
    "        driver.find_element_by_name('pass').send_keys(password)\n",
    "\n",
    "        # clicking on login button\n",
    "        driver.find_element_by_id('loginbutton').click()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"There's some error in log in.\")\n",
    "        print(sys.exc_info()[0])\n",
    "        exit()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    ids = [\"https://en-gb.facebook.com/\" + line.split(\"/\")[-1] for line in open(\"Desktop/input.txt\", newline='\\n')]\n",
    "\n",
    "    if len(ids) > 0:\n",
    "        # Getting email and password from user to login into his/her profile\n",
    "        email = input('\\nEnter your Facebook Email: ')\n",
    "        password = input('Enter your Facebook Password: ')\n",
    "\n",
    "        print(\"\\nStarting Scraping...\")\n",
    "\n",
    "        login(email, password)\n",
    "        scrap_messages()\n",
    "        scrap_profile(ids)\n",
    "        driver.close()\n",
    "    else:\n",
    "        print(\"Input file is empty..\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "\n",
    "# get things rolling\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
